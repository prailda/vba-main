---
title: "WebWatcher: Breaking New Frontiers of Vision-Language Deep Research Agent — полный обзор"
authors:
  - Xinyu Geng
  - Peng Xia
  - Zhen Zhang
  - Xinyu Wang
  - Qiuchen Wang
  - Ruixue Ding
  - Chenxi Wang
  - Jialong Wu
  - Yida Zhao
  - Kuan Li
  - Yong Jiang
  - Pengjun Xie
  - Fei Huang
  - Jingren Zhou (Tongyi Lab, Alibaba)
source: https://arxiv.org/pdf/2508.05748
date: 2025-09-03
tags:
  - Vision-Language
  - Deep-Research-Agent
  - WebAgent
  - VQA
  - RL
  - GRPO
  - BrowseComp-VL
summary: "Развёрнутый обзор статьи WebWatcher (Tongyi Lab, Alibaba): мотивация, pipeline генерации мультимодальных траекторий, SFT + GRPO дообучение, экспериментальные настройки и результаты на подборке бенчмарков (BrowseComp-VL, LiveVQA, HLE и др.)."
---

# WebWatcher — полный обзор (русский)

> Краткое содержание

WebWatcher — мультимодальный агент класса "deep research", который сочетает зрительно-языковое рассуждение (vision-language reasoning) и координацию множества внешних инструментов (поиск в web, посещение страниц, OCR, code interpreter и др.). Авторы предлагают: (1) pipeline генерации высококачественных мультимодальных многошаговых траекторий для обучения; (2) смешанное обучение: SFT на синтетических траекториях + ранжировочно-ориентированное RL (GRPO); (3) новый бенчмарк BrowseComp-VL и набор экспериментов, демонстрирующих значимый прирост эффективности по сравнению с RAG/paradigm и несколькими сильными моделями.

---

## Оглавление


- [[#Оглавление|Оглавление]]
- [[#1. Введение <a name="введение"></a>|1. Введение <a name="введение"></a>]]
- [[#2. Задача и требования к агенту <a name="задача"></a>|2. Задача и требования к агенту <a name="задача"></a>]]
- [[#3. Датасет и pipeline генерации (BrowseComp-VL) <a name="pipeline"></a>|3. Датасет и pipeline генерации (BrowseComp-VL) <a name="pipeline"></a>]]
	- [[#3. Датасет и pipeline генерации (BrowseComp-VL) <a name="pipeline"></a>#Цель создания датасета|Цель создания датасета]]
	- [[#3. Датасет и pipeline генерации (BrowseComp-VL) <a name="pipeline"></a>#Основные этапы pipeline|Основные этапы pipeline]]
	- [[#3. Датасет и pipeline генерации (BrowseComp-VL) <a name="pipeline"></a>#Итоги (порядковые величины)|Итоги (порядковые величины)]]
- [[#4. Архитектура агента и интегрированные инструменты <a name="architecture"></a>|4. Архитектура агента и интегрированные инструменты <a name="architecture"></a>]]
	- [[#4. Архитектура агента и интегрированные инструменты <a name="architecture"></a>#Инструменты (интеграция)|Инструменты (интеграция)]]
- [[#5. Обучение: SFT и GRPO (подробно) <a name="training"></a>|5. Обучение: SFT и GRPO (подробно) <a name="training"></a>]]
	- [[#5. Обучение: SFT и GRPO (подробно) <a name="training"></a>#5.1 SFT: cold-start и детали <a name="sft"></a>|5.1 SFT: cold-start и детали <a name="sft"></a>]]
	- [[#5. Обучение: SFT и GRPO (подробно) <a name="training"></a>#5.2 GRPO: Group-Relative Policy Optimization — формулы и псевдокод <a name="grpo"></a>|5.2 GRPO: Group-Relative Policy Optimization — формулы и псевдокод <a name="grpo"></a>]]
- [[#6. Экспериментальная часть: настройки и метрики <a name="experiments"></a>|6. Экспериментальная часть: настройки и метрики <a name="experiments"></a>]]
	- [[#6. Экспериментальная часть: настройки и метрики <a name="experiments"></a>#Данные / бенчмарки|Данные / бенчмарки]]
	- [[#6. Экспериментальная часть: настройки и метрики <a name="experiments"></a>#Модели / backbone|Модели / backbone]]
	- [[#6. Экспериментальная часть: настройки и метрики <a name="experiments"></a>#Метрики|Метрики]]
	- [[#6. Экспериментальная часть: настройки и метрики <a name="experiments"></a>#Вычислительная установка (кратко)|Вычислительная установка (кратко)]]
- [[#7. Результаты — ключевые таблицы и обсуждение <a name="results"></a>|7. Результаты — ключевые таблицы и обсуждение <a name="results"></a>]]
	- [[#7. Результаты — ключевые таблицы и обсуждение <a name="results"></a>#7.1 BrowseComp-VL — pass@1 (извлечённый срез)|7.1 BrowseComp-VL — pass@1 (извлечённый срез)]]
	- [[#7. Результаты — ключевые таблицы и обсуждение <a name="results"></a>#7.2 LiveVQA — pass@1|7.2 LiveVQA — pass@1]]
	- [[#7. Результаты — ключевые таблицы и обсуждение <a name="results"></a>#7.3 HLE (Humanity's Last Exam) — avg по поддисциплинам (excerpt)|7.3 HLE (Humanity's Last Exam) — avg по поддисциплинам (excerpt)]]
	- [[#7. Результаты — ключевые таблицы и обсуждение <a name="results"></a>#7.4 Вклад SFT и GRPO (ablation summary)|7.4 Вклад SFT и GRPO (ablation summary)]]
- [[#8. Ablation и анализ ошибок <a name="ablation"></a>|8. Ablation и анализ ошибок <a name="ablation"></a>]]
- [[#9. Ограничения, этика и практическое применение <a name="limits"></a>|9. Ограничения, этика и практическое применение <a name="limits"></a>]]
	- [[#9. Ограничения, этика и практическое применение <a name="limits"></a>#Ограничения|Ограничения]]
	- [[#9. Ограничения, этика и практическое применение <a name="limits"></a>#Этические замечания|Этические замечания]]
	- [[#9. Ограничения, этика и практическое применение <a name="limits"></a>#Практическое применение|Практическое применение]]
- [[#10. Репозиторий и ресурсы <a name="resources"></a>|10. Репозиторий и ресурсы <a name="resources"></a>]]
- [[#11. Заключение и направления дальнейшей работы <a name="conclusion"></a>|11. Заключение и направления дальнейшей работы <a name="conclusion"></a>]]
	- [[#11. Заключение и направления дальнейшей работы <a name="conclusion"></a>#Приложения (псевдокоды, формулы, дополнительные таблицы)|Приложения (псевдокоды, формулы, дополнительные таблицы)]]
		- [[#Приложения (псевдокоды, формулы, дополнительные таблицы)#A. Формулы (кратко)|A. Формулы (кратко)]]
		- [[#Приложения (псевдокоды, формулы, дополнительные таблицы)#B. Псевдокод: генерация траекторий для SFT|B. Псевдокод: генерация траекторий для SFT]]
		- [[#Приложения (псевдокоды, формулы, дополнительные таблицы)#C. Псевдокод: краткая схема работы агента при inference|C. Псевдокод: краткая схема работы агента при inference]]
- [[#Контактные заметки|Контактные заметки]]


---

## 1. Введение

Современные агенты для "deep research" (поиск, синтез знаний, составление сложных выводов) в основном опираются на текстовую информацию и RAG-пайплайны. При этом значительная часть научной и инженерной информации представлена в виде изображений (диаграммы, графики, интерфейсы, схемы), где требуется совместное зрительно-языковое рассуждение и многопеременное взаимодействие с внешними инструментами (открыть страницу, запустить OCR, выполнить вычисления). Авторы статьи формулируют проблему: как построить агента, способного надежно решать такие мультимодальные, multi-hop задачи, требующие как поиска в web, так и визуальной интерпретации.

WebWatcher — ответ на эту задачу: агента, обучаемый на многошаговых, grounded траекториях, с SFT-инициализацией и последующим дообучением через ранжировочно-ориентированную RL-методику (GRPO). Для оценки создан новый бенчмарк BrowseComp-VL, включающий вопросы, которые невозможно решить только по одному модальти (текст или изображение), и где нужен синтез информации.

## 2. Задача и требования к агенту <a name="задача"></a>

Ключевые свойства целевых задач:

* **Мультимодальность**: имеются как тексты, так и изображения, которые дополняют друг друга.
* **Multi-hop / grounded**: требуется комбинация нескольких источников информации и многошаговая цепочка рассуждений (последовательные вызовы инструментов).
* **Интерактивность**: агент должен уметь вызывать инструменты (Search, Visit page, OCR, Code Interpreter) и корректно интерпретировать получаемые наблюдения.
* **Robustness to noisy web content**: web-страницы содержат лишнюю, изменчивую или вводящую в заблуждение информацию — агент должен фильтровать и выбирать релевантное.

## 3. Датасет и pipeline генерации (BrowseComp-VL) <a name="pipeline"></a>

### Цель создания датасета

Создать набор задач, требующих joint vision-language reasoning и навигации по web. Авторы следуют идеям BrowseComp (текстовый BrowseComp) и расширяют их на мультимодальные сценарии (BrowseComp-VL).

### Основные этапы pipeline

1. **Seed collection**: выбор root-источников (arXiv, GitHub, Википедия и т.п.) для генерации богатых контекстов.
2. **Text QA generation**: генерация многошаговых текстовых QA (multi-hop) путём random-walk по документам и вызова сильного LLM для генерации вопросов и золотых ответов.
3. **QA → VQA трансформация**: для тех сущностей, где визуальный контекст релевантен, извлекаются реальные изображения (поиск через Google SerpApi или аналог), формируются визуально обусловленные варианты вопросов — включая маскирование/фаззинг (fuzzing) вместо явных наименований, чтобы избежать прямого поиска по ключевым словам.
4. **Фильтрация качества**: несколько этапов автоматической валидации с участием LLM (selector/examiner), оценивающих соответствие изображения и вопроса, возможность ответить по видимому контенту, и отбрасывающих некорректные случаи.
5. **Структурирование уровней сложности**: Level-1 (явные сущности, multi-hop), Level-2 (замаскированные сущности, harder reasoning).

### Итоги (порядковые величины)

* Генерация: исходно десятки тысяч QA; после фильтрации — сотни тысяч примеров на разных уровнях. Для SFT оставлено ~8k высококачественных траекторий (см. раздел обучения).

## 4. Архитектура агента и интегрированные инструменты <a name="architecture"></a>

WebWatcher — модель-агент, которая принимает на вход multimodal observation (текст + изображение) и генерирует последовательность действий в ReAct-стиле: *thought → tool_call → observation → thought → … → final_answer*.

### Инструменты (интеграция)

* **Web Image Search** (через SerpApi или аналог) — поиск релевантных изображений.
* **Web Text Search** — полнотекстовый web-поиск.
* **Visit Page** (web crawling / page rendering) — посещение веб-страниц и извлечение фрагментов (Jina / internal crawler).
* **OCR** — распознавание текста на изображениях/страницах.
* **Code Interpreter** — выполнение вычислений, парсинга данных, извлечение таблиц.
* **Local tools** — логирование, кеширование, ранжирование candidate pages/images.

Агент обучается прогнозировать не только текстовую следующее мысль, но и инструментальные вызовы (какой инструмент, с каким аргументом) и интерпретацию полученных наблюдений.

## 5. Обучение: SFT и GRPO (подробно) <a name="training"></a>

Обучение состоит из двух этапов:

1. **SFT (Supervised Fine-Tuning)** — холодный старт на высококачественных синтетических траекториях (prediction of next tool call / thought). Иллюстративно: dataset ~8k траекторий с разной длиной (минимум 3 вызова инструментов), каждая траектория — последовательность действий + наблюдений + итоговый ответ.

2. **GRPO (Group-Relative Policy Optimization)** — RL-фаза, оптимизирующая policy с учётом ранжирования траекторий в группе: для одного задания генерируется набор кандидатов траекторий G (различные политики/вариации), затем применяется group-relative advantage для обновления policy таким образом, чтобы выигрывать внутри группы.

### 5.1 SFT: cold-start и детали <a name="sft"></a>

* **Цель**: научить модель корректно формировать многошаговые цепочки вызовов инструментов и минимально рабочую логику рассуждения.
* **Формат**: supervised learning с кросс-энтропийной потерей по токенам; модель предсказывает последовательность, включая маркеры инструментов и аргументы.
* **Данные**: ~8k высококачественных траекторий, дополнительно 2k VQA для выравнивания.
* **Гиперпараметры (примерные, из статьи)**: обучение на Qwen2.5-VL-7B / 32B бэконах; temperature=0.0 для deterministic decoding на eval; оптимизатор — AdamW; батч ~ 32; lr ~ 1e-5 (примерно, точные значения в статье).

### 5.2 GRPO: Group-Relative Policy Optimization — формулы и псевдокод <a name="grpo"></a>

**Идея**: вместо классического PPO, где каждая траектория получает scalar reward, GRPO формулирует сравнение внутри группы траекторий G = {τ_1, …, τ_N} для одной и той же задачи. Цель — повышать вероятность лучших траекторий относительно худших внутри группы.

**Вычисление награды**

Авторы комбинируют два компонента качества итоговой траектории:

* $r_f(\tau)$ — формальная бинарная метрика (0/1) корректности (например, ответ совпадает с золотым).
* $r_a(\tau)$ — семантическая оценка (в диапазоне [0,1]) от LLM-града/оценщика (оценка качества ответа/согласованности).

Комбинированная награда:

[ R(\tau) = w \cdot r_f(\tau) + (1 - w) \cdot r_a(\tau) ]

где $w$ — весовой коэффициент (в статье $w = 0.2$ по умолчанию).

**Group-relative advantage (интуитивное описание)**

Для группы G вычисляется средняя и дисперсия наград по группе; advantage для траектории τ_i определяется относительно группы:\

[ A_{rel}(\tau_i) = R(\tau_i) - \frac{1}{N} \sum_{j=1}^N R(\tau_j) ]

(в статье применяется более стабильная нормализация и clipping, вдохновлённая PPO — см. текст для деталей).

**Целевая функция (упрощённо)**

Обновление policy формулируется как максимизация ожидаемой group-relative advantage при ограничении изменения policy (KL/ratio clipping):

[ L(\theta) = - \mathbb{E}*{\tau \sim \pi*{\theta_{old}}} \left[ \min\left( r(\theta) A_{rel}(\tau), \mathrm{clip}(r(\theta), 1-\epsilon, 1+\epsilon) A_{rel}(\tau) \right) \right] ]

где $r(\theta) = \frac{\pi_{\theta}(\tau)}{\pi_{\theta_{old}}(\tau)}$ — отношение вероятностей (policy ratio), и $\epsilon$ — коэффициент клиппинга.

**Псевдокод GRPO (упрощённый)**

```
for each epoch:
  for each task sample:
    generate group G = {tau_1, ..., tau_N} by sampling policy (or using temperature/sample variations)
    compute R(tau_i) for each tau_i (R = w * r_f + (1-w) * r_a)
    compute A_rel(tau_i) = R(tau_i) - mean_j R(tau_j)
    compute policy ratios r(theta) = pi_theta(tau_i) / pi_theta_old(tau_i)
    compute clipped objective L according to formula
    update theta via gradient descent on L
  end
end
```

**Примечания**:

* Авторы выбирают групповой размер $N=16$ и $w=0.2$ как рабочие настройки; используются батчевые обновления и normalizing/standardizing для A_rel.
* GRPO направлен на то, чтобы улучшать policy в смысле ранжирования кандидатов внутри группы, что особенно полезно для задач, где абсолютная бинарная награда редка/шумна.

## 6. Экспериментальная часть: настройки и метрики <a name="experiments"></a>

### Данные / бенчмарки

* **BrowseComp-VL** — авторский бенчмарк (multi-modal, multi-hop), уровни сложности Level-1 / Level-2.
* **LiveVQA**, **HLE (Humanity's Last Exam)**, **MMSearch**, **SimpleVQA** и др. — для оценки широкого спектра навыков агента.

### Модели / backbone

* Qwen2.5-VL-7B и Qwen2.5-VL-32B как основные backbones для SFT/GRPO экспериментов; сравнение с Gemini, GPT-4o и другими.

### Метрики

* **pass@1** — основная метрика корректности (доля случаев, когда первый сгенерированный ответ считается правильным).
* **LLM-as-judge** — автоматическая проверка качества/семантической точности с помощью сильного LLM-оценщика.
* **Анализ траекторий** — средняя длина, число вызовов инструментов, доля корректных инструментальных шагов.

### Вычислительная установка (кратко)

* Обучение на GPU-кластере (multi-GPU), оптимизация по AdamW.
* Дегенерация: temperature=0.6 / top-p=0.95 для аналитики; evaluation с temperature=0.0 для repeatable results.

## 7. Результаты — ключевые таблицы и обсуждение <a name="results"></a>

> Ниже приведены извлечённые сводные таблицы (интерпретация автора). Таблицы сокращены для обзора; полные числа — в оригинальной статье.

### 7.1 BrowseComp-VL — pass@1 (извлечённый срез)

| Модель / метод            | pass@1 (%) |
| ------------------------- | ---------: |
| WebWatcher (SFT+GRPO)     |   **58.7** |
| Qwen2.5-VL-32B (baseline) |       41.3 |
| Gemini-2.5-flash          |       35.7 |
| Qwen2.5-VL-72B            |       34.0 |
| GPT-4o (RAG)              |       30.3 |

**Комментарий:** WebWatcher демонстрирует значительный прирост по сравнению с версиями только LLM + RAG.

### 7.2 LiveVQA — pass@1

| Модель         | pass@1 (%) |
| -------------- | ---------: |
| WebWatcher     |   **55.3** |
| Qwen2.5-VL-32B |       43.9 |
| Claude-3.7     |       32.7 |
| Qwen2.5-VL-72B |       29.2 |
| GPT-4o         |       24.1 |

### 7.3 HLE (Humanity's Last Exam) — avg по поддисциплинам (excerpt)

| Парадигма / Модель                      |  Avg (%) |
| --------------------------------------- | -------: |
| Direct Inference — GPT-4o               |      6.5 |
| RAG — GPT-4o                            |     12.3 |
| Reasoning model — Gemini-2.5-Pro        |     15.8 |
| Open source agent — OmniSearch (GPT-4o) |      9.3 |
| WebWatcher-7B                           |     10.6 |
| WebWatcher-32B                          | **13.6** |

**Комментарий:** на HLE WebWatcher показывает конкурентоспособные, но не прорывные абсолютные значения — задачи чрезвычайно сложные.

### 7.4 Вклад SFT и GRPO (ablation summary)

* **SFT на качественных траекториях** даёт значительный прирост в корректности инструментальных шагов и общей pass@1: улучшение порядка 10–15 п.п. (зависит от бенчмарка).
* **GRPO** даёт дополнительную выгоду: порядка 4–8 п.п. на BrowseComp-VL и LiveVQA (особенно ощутимо в Level-2 задачах), улучшая ранжирование и стабилизируя выбор инструментальных стратегий.

## 8. Ablation и анализ ошибок <a name="ablation"></a>

Авторы приводят несколько важных наблюдений:

* **Качество траекторий**: quantity ≠ quality — малое количество высококачественных траекторий для SFT лучше, чем большое количество низкокачественных.
* **Инструменты**: Visit + OCR + Code Interpreter особенно важны для задач, где изображение содержит текст/таблицы или где требуется дополнительный парсинг.
* **LLM-оценщик**: автоматические оценки семантической точности зависят от качества LLM-оценщика и могут вносить систематические искажения.

Типичные ошибки:

1. **Мис-интерпретация изображения** — локализация сущности неверна или OCR возвращает ошибочные фрагменты.
2. **Неверные навигационные шаги** — агент посещает нерелевантные страницы из-за слабого ранжирования candidate pages.
3. **Ошибки агрегации информации** — корректные локальные факты не объединяются в глобальный ответ.

## 9. Ограничения, этика и практическое применение <a name="limits"></a>

### Ограничения

* Зависимость от внешних инструментов (качество поиска, доступность страниц).
* Риск ошибок из-за устаревшей/некорректной информации в web-ресурсах.
* Авторы используют LLM-as-judge — возможна систематическая ошибка в автоматическом оценивающем модуле.

### Этические замечания

* Валидация фактов в автоматизированных системах должна учитывать возможную предвзятость источников и права на использование контента (копирайт веб-страниц и изображений).

### Практическое применение

* Автоматический разбор научных статей (диаграммы, формулы).
* Интеллектуальный ассистент для инженеров/исследователей: поиск, агрегация и интерпретация мультимодальной информации.
* Мониторинг изображений и визуального контента в enterprise-сценариях.

## 10. Репозиторий и ресурсы <a name="resources"></a>

* Репозиторий, указанный авторами: `https://github.com/Alibaba-NLP/WebAgent` (см. статью для деталей и возможного кода/датасетов).
* Статья (arXiv): `https://arxiv.org/pdf/2508.05748`.

## 11. Заключение и направления дальнейшей работы <a name="conclusion"></a>

WebWatcher показывает, что комбинация качественного data-synthesis pipeline (генерация многошаговых мультимодальных траекторий), SFT и специализированного RL (GRPO) позволяет существенно поднять эффективность агента в задачах, требующих joint vision-language reasoning и web-navigation. Основные перспективы:

* Увеличение объёма и качества аннотированных траекторий (включая человеческую ревизию) для ещё лучшего SFT.
* Улучшение модулей ранжирования и retrieval (меньше мусора → меньше неверных навигаций).
* Разработка более надежных и прозрачных методов оценки качества (человеческая разметка + robust LLM-judges).

---

### Приложения (псевдокоды, формулы, дополнительные таблицы)

#### A. Формулы (кратко)

* Комбинированная награда: $R(\tau) = w \cdot r_f(\tau) + (1 - w) \cdot r_a(\tau)$.
* Group-relative advantage: $A_{rel}(\tau_i) = R(\tau_i) - \frac{1}{N} \sum_{j=1}^N R(\tau_j)$.
* Клиппинг-объектив (по аналогии с PPO): $L = -\mathbb{E}[\min(r(\theta) A_{rel}, \mathrm{clip}(r(\theta), 1-\epsilon, 1+\epsilon) A_{rel})]$.

#### B. Псевдокод: генерация траекторий для SFT

```
for each QA sample:
  seed = random_walk(root_url, depth)
  context = extract_text_and_images(seed)
  generate_candidate_QA_with_LLM(context)
  for each candidate:
    retrieve_images_for_entities(candidate)
    mask_entities_if_needed (fuzzing)
    formulate VQA question
    validate candidate via selector/examiner LLM
    if pass_quality_checks:
      store trajectory: [thought, tool_call, observation, ... , final_answer]
```

#### C. Псевдокод: краткая схема работы агента при inference

```
input: question q, optional image I
memory = []
while not finished and steps < max_steps:
  thought = model.generate_thought(q, I, memory)
  if thought.suggests_tool_call:
    obs = call_tool(thought.tool, thought.args)
    memory.append(obs)
  else if thought.produces_answer:
    return answer
  else:
    continue
```

---

